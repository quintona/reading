\documentclass{article}

\input{base.tex}


\begin{document}

\section{Ideas}

Take the concept of the lambda architecture to the next logical level, which is using combining batch and realtime DBs, together with combining eventual with strongly consistent databases, and expose them via a single query planner and interface. Thus expose a DB that beats the CAP theorem entirely. 

A generalised query layer that allows you to query across database. This would be the generalisation of the idea above, and obivously more generally applicable. The query planner would be really interesting, especially given notional materialized views across database.

\section{Notes on Papers}

\subsection{Database}

\subsubsection{LINVIEW: Incremental View Maintenance for Complex Analytical
               Queries}

\textbf{Article reference:} \cite{DBLP:journals/corr/NikolicEK14}

The article has 2 interesting aspects, firstly they present a framework for incremental maintenance of algrebra applications. Incremental in the sense that incremental materialized views can be generated as an optimisation, especially in cases where expensive matrix multiplication is involved. 

Secondly the article presents a compiler that generates spark code that implements the algebra logic which is defined in either R or some other language. 

\textbf{Thoughts:} There are some high level take aways from this, which I need to consider:
\begin{itemize}
  \item There is an entire world of research into incremental optimisations. This will be required in order to fully support a lambda approach, I suspect. In the high level idea of combining different DBs of different attributes, various optimisation techniques will be required out of the box
  \item The notion of generating this kind of code is also interesting, I would need to do some more reading into compiler theory, or rather macros within scala. I doubt I would build a compiler toolchain, but rather use standard scala structures for this. 
\end{itemize}

\subsubsection{Shark: SQL and Rich Analytics at Scale}

\textbf{Article Reference:} http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-214.pdf

They have thought really long and hard about the various optimisations that could be applied. It also serves as a really nice summary of the various technologies and techniques in this space. Given that shark project is dead, they obviously realized that trying to get the hive code to play nicely is quite tough, they also realized that getting all these techniques up to industrial standards is quite tough, all that being said, they have proposed some excellent ideas and I assume many of them are being dragged through to Spark SQL given that it the next generation of whats in the paper. 

The availability model is also an interesting dimension to consider. 

I don't hold much hope in the distributed RDBMS model, but it is interesting to contrast it, and potentially there are some lessons to learn there. I also need to search more on the traditional MPP DB model.


\subsubsection{Exploiting Opportunistic Physical Design in Large-scale
               Data Analytics}

\textbf{Article Reference:} \cite{DBLP:journals/corr/abs-1303-6609}

\subsubsection{MISO: souping up big data query processing with a multistore system}

\textbf{Arcticle Reference:} \cite{LeFevre:2014:MSU:2588555.2588568}

Multistore systems utilize multiple distinct data stores such as Hadoop's HDFS and an RDBMS for query processing by allowing a query to access data and computation in both stores. Current approaches to multistore query processing fail to achieve the full potential benefits of utilizing both systems due to the high cost of data movement and loading between the stores. Tuning the physical design of a multistore, i.e., deciding what data resides in which store, can reduce the amount of data movement during query processing, which is crucial for good multistore performance. In this work, we provide what we believe to be the first method to tune the physical design of a multistore system, by focusing on which store to place data. Our method, called MISO for MultISstore Online tuning, is adaptive, lightweight, and works in an online fashion utilizing only the by-products of query processing, which we term as opportunistic views. We show that MISO significantly improves the performance of ad-hoc big data query processing by leveraging the specific characteristics of the individual stores while incurring little additional overhead on the stores.

\textbf{Thoughts:} This is exactly in line with what I was thinking, with a focus on the consistency aspects and some algrebras.

\subsubsection{SPANStore: Cost-effective Geo-replicated Storage Spanning Multiple Cloud Services}

\textbf{Arcticle Reference:} \cite{Wu:2013:SCG:2517349.2522730}

\subsubsection{Indexing HDFS Data in PDW: Splitting the data from the index}

\textbf{Article Reference:} http://www.vldb.org/pvldb/vol7/p1520-gankidi.pdf

A relational DB is used to effectively store the index for a table in HDFS. This is a rather simple idea, but the complexity comes in the query planning and the cost optimiser, which needs to balance the cost benefit of indexing the data versus simply mapping over the dataset. They do some of this analysis in the paper. It would be interesting to generalize the idea, the issue is the query planner that will span the technologies, I need to give this more thought, there is also the UBTree approach would could come into play and should also be considered in the cost based approach. Also probablistic data structures...

I think the fundamental that people miss is that HDFS should remain the underlying storage, because you can always run full table scans over the data. It becomes the lowest common denominator, and this gives you the ultimate flexibility and reliability. Externalizing indexes, and other optimisation should sit along side, as long as they don't get sticky and break the autonomy of the data in such a way that mapping isn't possible. There are various ways to externalize indexes into the colunmar data in HDFS, the important issue is having a query planner that is pluggable enough to allow for these different mechanisms. Once you have such a query planner (spark SQL is a good starting point), then it is important to consider all the intended use cases and therefore the different required consistency levels. This is where the lambda portion would come in and the consistency discussion becomes quite interesting. Think about the Druid architecture, plus HDFS with externalized index and query planner. Could be onto something there...

It is interesting that the index size was roughly 10 percent of the total data size. That gives one a real simple cost model to work against in terms of the relational DB that supports indexing. It also means that practically you can't really scale the indexes that much, unless maybe you use some more scalable DB for the indexes, this is where it would be interesting to use SOLR or related for the task. The trick of course being that you could easily get to a similar size to the actual data, but there are ways to control/manage that risk. 

\subsubsection{Mesa: Geo-Replicated, Near Real-Time, Scalable Data Warehousing}

\textbf{Article Reference:} \cite{42851}

The google guys, in their usual style, are mostly bragging more than providing insight in this one. The operational complexity involved in their proposed solution is so massive, there are only a hand full of teams in the world that could possibly consider it, and they mostly work for google anyway. 

That being said, there is a nice little bit of validation in my thinking at the end of the article: "Mesa explores a new point in the design space with high scalability, strong consistency, and transactional guarantees by restricting the system to be only available for batched and controlled updates that are processed in near real-time" 

Constraints have to be removed, rethinking the way data gets into the DB and made available is the place to focus. It also validates the basic Lambda/Duid type model.

\subsection{Big Data}

\subsubsection{Summingbird: A Framework for Integrating Batch and Online MapReduce Computations}

\textbf{Article Reference:} \href{http://www.vldb.org/pvldb/vol7/p1441-boykin.pdf}

\textbf{Notes:}

Commutative and associative get used too interchangably in the article. ``Commutativity is needed because partial results from each partition may be aggregated in arbitrary order. In the online processing case, commutativity lets us handle issues like out-of-order message delivery. For both online and batch processing, commutativity further enables certain optimizations such as combiners in MapReduce and partial aggregations in Storm (as discussed above)`` This all makes sense, except it is preceeded with a definition of semi-group, which is associative. I need to do a write up here on the differences in these contexts, so as not to get myself confused.

Accoring to the article: ``Commutativity is needed because partial results from each partition may be aggregated in arbitrary order.
In the online processing case, commutativity lets us handle issues like out-of-order message delivery. For both online and batch processing, commutativity further enables certain optimizations such as combiners in MapReduce and partial aggregations in Storm (as discussed above)'' Now my category theory is a bit rusty, but I thought monoid was a semi-group with an identity element, this is the first time I have seen the commutativity property added to the category... More reading required there. Ah, nevermind, they are referring to a commutative monoid or a commutative semi-group. I wonder if that is a seperate category, which laws would you assert with it? 

They force the time invariance at the API level, which makes the merging logic easy. Fair enough. 

"The data import pipeline is engineered so that a directory does not appear until all the files contained in that directory have arrived, so it is not possible to process partially-imported results". Need to read up on their logging infrastructure.

\textbf{Summary:} Overall I think their ideas are quite sound. I need to play with the API to understand what happens with the edge cases where your can't express things in a commutative category, and need to dive down into specific primitives to achieve what you need to achieve. The boilerplate of combining the aggregate values and launching MR jobs when required, has to be done and so is tempting to leverage, however I am still not convinced these make the added complexity a worthwhile trade off, especially when you can view the bulk of the logic as monoids that are entirely library with law based tests. The time invariant and combining streams aspect, we should learn from them.

They also have some nice optimisations like fusing or colocating functionality in the spout and bolt in order to remove the serialisation overheads. We should learn from this.

\subsection{Algebra}

\subsubsection{Hardvard course notes}

\begin{itemize}
  \item Once you have proved associativity of 3 elements, it can be proved that it holds for all elements in the set. 
  \item The inverse of the group: Given the identity of the group $ e $ then the inverse is $a \circ a^{-1} = a^{-1} \circ a = e$
  \item the Identity element of the bijection is the set that maps to itself (set plus product)
  \item automorphism == bijection
  \item You can talk of subsets of a group $g_{a} \subset g_{b}$
  \item factorial feels like the opposite of logorithmic. Not quite, but there feels like a relationship
  \item A group has a min number of elements, which is 1, the identity.
  \item Do you talk about a group as relating to or containing a set? 
  \item An Abilian group is commutative
  \item transposition simply means swapping the positions of the elements in the set
  \item Composition notation to watch out for: $ab(1) = a(b(1))$
  \item you can use a sub group to prove properties of a group without having to do the entire computation of the group to make the proof
  \item what is the precise meaning of "fixing something"?  Also stabilize? Meaning make something constant in some vector space?
  \item A group has a set of structure preserving bijections
  \item when we talk about ``Order'', it refers to the number of cyclic instances under some power
  \item Relabling the type under a given operator is called isomorphism, does that make the type different? So isomorphism is $f(x \circ y) = f(x) \circ f(y)$ How do I think about this in terms of types? $f(M[_](x) \circ M[_])$ ??? Isomorphism has to be between types that have the same multiplication table. What are the laws of a group again? 
  \item every 2 cyclic groups of order n are isomorphic
  \item A cyclic group is a group that contains some element that generates the entire group
  \item {R,+} is isomorphic to {R(>0),x}. I need to prove this to myself in code. good chance to do so.
  \item A group contains elements that all have an inversion, a semigroup contains elements that don't
  \item Properties to test if groups are isomorphic: 
    \item they have the same cardinality
    \item If G1 is abelian, then G2 must be
    \item G1 and G2 have the same number of elements of every order
  \item Isomorphism between the group and itself is the automorphism. Also called the structure preserving maps or simitries.
  \item Fact: inverses are automorphisms
  \item bijections only require the same cardinality
  \item Every isomorphism is a homomorphism
  \item Homomorhism def: is a function $f: G1 \rightarrow G2$ such that $f(xy) = f(x) \circ f(y)$. homomorphism is a structure-preserving map. It preserves the multiplication between the groups. 
  \item Isomorphism is a homomorphism, but it changes the representational aspects of the elements
  \item Image == column space of a matrix from linear algebra
  
\end{itemize}

\subsubsection{Distributivity versus associativity in the homology theory of algebraic structures}

\textbf{Article Reference:} \cite{2011arXiv1109.4850P}

\subsection{programming}

\subsubsection{General thoughts}

\begin{itemize}
  \item Can't define a functor for set, because of the constraint on the type. Thus coyoneda. 
  \item Functor is something you can define a map on. 
  \item Monad is something you can define a flatmap on (bind)
  \item Once you have a functor, you can construct a free monad
  \item An identity is used to start things unravelling. Recursive case, at the next level. 
\end{itemize}

\subsubsection{Reflection without remorse}

\textbf{Article Reference:} \cite{conf/haskell/PloegKiselyov14}

\textbf{From Article:}

A series of list appends or monadic binds for many monads performs algorithmically worse when left-associated. Continuation- passing style (CPS) is well-known to cure this severe dependence of performance on the association pattern. The advantage of CPS dwindles or disappears if we have to examine or modify the intermediate result of a series of appends or binds, before continuing the series. Such examination is frequently needed, for example, to control search in non-determinism monads.

For example, in the expression (x ++ y) ++ z, the list x must be traversed twice: it occurs twice in a left hand side argument to +. Hence, this expression runs in in 2|x| + |y| + 2 steps, whereas the equivalent expression x ++ (y ++ z) runs in just |x| + |y| + 2 steps. In this way, a wrong grouping of expressions involving ++ can easily lead to severe performance problems. 

In general, the problem occurs with any associative (or satisfying the associativity monad law) operator (⊕) that traverses its left argument but not its right argument that operates on some recursive2 data type. In this situation, (x ⊕ y) ⊕ z costs |x| more steps to eval- uate than x ⊕ (y ⊕ z), where |x| is now the number of values of type X inside x that are non-terminal (i.e. they are not for example the empty list or a leaf). Repeated application of such an operator can lead to asymptotic running time overhead if |a⊕b| ≥ |a|+|b|

We cannot expect the programmer to only form right- associated expressions, especially when using laziness: the pro- grammer must then make sure that every time the operator is used, the left hand side cannot be itself a result of this operator.

A benefit of using type aligned sequences in this way, instead of directly using regular sequences, is that type aligned sequences rule out a class of implementation bugs: the types in a type aligned sequence enforce the ordering of the elements. Hence, accidentally switching two elements will result in a type error, as the resulting sequence may not be a path. In contrast, in regular sequences the types do not enforce the ordering of the elements and an accidental change of order in, for instance, the definition of concatenation would have gone unnoticed by the type checker.

\textbf{Thoughts:}

So the key issue is that the left associativity, under certain circumstances can cause a real performance problem. CPS is often used as a means to deal with this issue, however we can't inspect the operations (reflect) without great cost even when using CPS, and in the case of Monadic contexts this is often extremely useful (think reactive). Reflect essentially means alternate between observing and building.

I am clearly missing something with regards to laziness and left associativity. Must go find some more information on this front. 

Type aligned sequences and free is worth reading more on. The type aligned conept in terms of operation ordering is really interesting, especially when we mix it with the concepts of planning and function fusion. Describing the computation where you strongly determine the order.

TAS - key point is to reify the functions, the composition is problem when you combine too many flatmap.

\subsubsection{Lift}

\textbf{Taken from}: \href{http://stackoverflow.com/questions/17965059/what-is-lifting-in-scala}{Stack Overflow}

There are a few usages:

\textbf{PartialFunction}


Remember a `PartialFunction[A, B]` is a function defined for some subset of the domain `A` (as specified by the `isDefinedAt` method). You can "lift" a `PartialFunction[A, B]` into a `Function[A, Option[B]]`. That is, a function defined over the *whole* of `A` but whose values are of type `Option[B]`

This is done by the explicit invocation of the method `lift` on `PartialFunction`.

\begin{minted}{scala}
    scala> val pf: PartialFunction[Int, Boolean] = 
          { case i if i > 0 => i % 2 == 0}
    pf: PartialFunction[Int,Boolean] = <function1>

    scala> pf.lift
    res1: Int => Option[Boolean] = <function1>

    scala> res1(-1)
    res2: Option[Boolean] = None

    scala> res1(1)
    res3: Option[Boolean] = Some(false)
\end{minted}

\textbf{Methods}

You can "lift" a method invocation into a function. This is called *eta-expansion* (thanks to Ben James for this). So for example:

\begin{minted}{scala}
    scala> def times2(i: Int) = i * 2
    times2: (i: Int)Int
\end{minted}

We lift a method into a function by applying the \textbf{underscore}

\begin{minted}{scala}
    scala> val f = times2 _
    f: Int => Int = <function1>

    scala> f(4)
    res0: Int = 8
\end{minted}

Note the fundamental difference between methods and functions. `res0` is an \textbf{instance} (i.e. it is a \textbf{value}) of the (function) type `(Int => Int)`

\textbf{Functors}

A *functor* (as defined by **scalaz**) is some "container" (I use the term *extremely* loosely), `F` such that, if we have an `F[A]` and a function `A => B`, then we can get our hands on an `F[B]` (think, for example, `F = List` and the `map` method)

We can encode this property as follows:

\begin{minted}{scala}
    trait Functor[F[_]] { 
      def map[A, B](fa: F[A])(f: A => B): F[B]
    }
\end{minted}

This is isomorphic to being able to "lift" the function `A => B` into the domain of the functor. That is:

\begin{minted}{scala}
    def lift[F[_]: Functor, A, B](f: A => B): F[A] => F[B]
\end{minted}

That is, if `F` is a functor, and we have a function `A => B`, we have a function `F[A] => F[B]`. You might try and implement the `lift` method - it's pretty trivial.

\textbf{Monad Transformers}

As *hcoopz* says below (and I've just realized that this would have saved me from writing a ton of unnecessary code), the term "lift" also has a meaning within **Monad Transformers**. Recall that a monad transformers are a way of "stacking" monads on top of each other (monads do not compose).

So for example, suppose you have a function which returns an `IO[Stream[A]]`. This can be converted to the monad transformer `StreamT[IO, A]`. Now you may wish to "lift" some other value an `IO[B]` perhaps to that it is also a `StreamT`. You could either write this:
\begin{minted}{scala}
    StreamT.fromStream(iob map (b => Stream(b)))
\end{minted}

Or this:
\begin{minted}{scala}
    iob.liftM[StreamT]
\end{minted}    

this begs the question: *why do I want to convert an `IO[B]` into a `StreamT[IO, B]`?*. The answer would be "to take advantage of composition possibilities". Let's say you have a function `f: (A, B) => C`

\begin{minted}{scala}
    lazy val f: (A, B) => C = ???
    val cs = 
      for {
        a <- as                //as is a StreamT[IO, A]
        b <- bs.liftM[StreamT] //bs was just an IO[B]
      }
      yield f(a, b)

    cs.toStream //is a Stream[IO[C]], cs was a StreamT[IO, C]
\end{minted}

\bibliographystyle{plain}
\bibliography{articles.bib}
Reflection without Remorse
